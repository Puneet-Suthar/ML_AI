{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ai Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules and libraries\n",
    "import speech_recognition as sr  # for speech recognition\n",
    "import tensorflow as tf          # deep learning library\n",
    "import cv2                       # computer vision library\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "from gtts import gTTS            # text to speech\n",
    "import pygame                    # multimedia library\n",
    "import webbrowser                # for web operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line if you don't have pyaudio installed. \n",
    "# It is required for microphone functionality of the speech_recognition library.\n",
    "# ! pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing out the available microphones and their indices.\n",
    "# You can find the device index form the output below which helps you to feed right device index to connect desired microphone.\n",
    "for index, name in enumerate(sr.Microphone().list_microphone_names()):\n",
    "    print(\"Microphone with name \\\"{1}\\\" found for `Microphone(device_index={0})`\".format(index, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_to_user():\n",
    "    \"\"\"\n",
    "    Listens to the user's speech and tries to convert it to text using Google's Speech Recognition.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: The recognized text from the user's speech or None if the recognition failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the recognizer object from the speech_recognition library.\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    # We specify the microphone we want to use by its index. Here, the microphone at index 3 is chosen.\n",
    "    # This could be any microphone recognized by the system, and its index can be found using \n",
    "    # the sr.Microphone().list_microphone_names() method.\n",
    "    with sr.Microphone(device_index=3) as source:\n",
    "        \n",
    "        print(\"listening...\")  # Notify the user that the system is ready to listen.\n",
    "        \n",
    "        # The listen method captures audio from the source (microphone) until silence is detected.\n",
    "        audio = r.listen(source)\n",
    "        \n",
    "        try:\n",
    "            # Convert the captured audio to text using Google's speech recognition API.\n",
    "            text = r.recognize_google(audio)\n",
    "            \n",
    "            return text  # Return the recognized text.\n",
    "            \n",
    "        except sr.UnknownValueError:\n",
    "            # If the speech is not clear or not recognized, this error will be raised.\n",
    "            print(\"Muje samaj nahi aaya\")  \n",
    "            \n",
    "            return None  # Return None if recognition failed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "1. `sr.Recognizer()`: This creates a new recognizer instance which is the main workhorse of the `speech_recognition` library.\n",
    "\n",
    "2. `sr.Microphone(device_index=3)`: The library supports multiple microphones. Here, we're selecting the microphone with index 3. This might be different on different systems, and you may need to enumerate the microphone list (as shown in the original notebook) to choose the correct index.\n",
    "\n",
    "3. `r.listen(source)`: The recognizer listens to the source (in this case, our microphone) and captures the audio.\n",
    "\n",
    "4. `r.recognize_google(audio)`: This sends the audio data to Google's Web Speech API for recognition. Google then returns the recognized text.\n",
    "\n",
    "5. `except sr.UnknownValueError`: If Google's API is unable to recognize the speech, this error is raised. In this code, a message is printed to indicate this, but in a production system, you'd probably want more sophisticated error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function to see if it captures our voice correctly.\n",
    "# listen_to_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading weights for the EfficientNetB3 model pre-trained on ImageNet\n",
    "efficientnet = tf.keras.applications.EfficientNetB3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image():\n",
    "    \"\"\"\n",
    "    Captures an image using the camera, preprocesses it, \n",
    "    and uses EfficientNetB3 to predict and describe the image's content.\n",
    "    \n",
    "    Returns:\n",
    "        str: A description of the content in the captured image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializes the video capture with the camera at index 2. This will connect to the third camera \n",
    "    # (indexing starts at 0). If you have multiple cameras, you might need to change this number.\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    \n",
    "    # Capture a single frame/image from the video source (camera in this case). \n",
    "    # 'flag' indicates if the capture was successful, 'img' contains the captured frame.\n",
    "    flag, img = cap.read()\n",
    "    \n",
    "    # Convert the color format of the image from BGR (Blue-Green-Red, which is OpenCV's default) \n",
    "    # to RGB (Red-Green-Blue), which is more standard and used by many other libraries.\n",
    "    img = img[:,:,::-1]\n",
    "    \n",
    "    # Display the captured image using matplotlib.\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Resize the image to 300x300 pixels, which may be the input size expected by the EfficientNetB3 model.\n",
    "    img_array = cv2.resize(img, (300,300))\n",
    "    \n",
    "    # Expand the dimensions of the image. This is required because the neural network expects a batch of images,\n",
    "    # not a single image. By expanding dimensions, we're essentially converting the single image into a batch of one image.\n",
    "    img_array = tf.expand_dims(img_array, axis = 0)\n",
    "    \n",
    "    # Preprocess the image array to ensure it's in the format the EfficientNetB3 model expects.\n",
    "    # This might involve normalizing pixel values, zero-centering, etc.\n",
    "    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n",
    "    \n",
    "    # Use the EfficientNetB3 model to predict the content of the image. \n",
    "    # This returns a probability distribution over all categories in the model's training data.\n",
    "    prediction = efficientnet.predict(img_array)\n",
    "    \n",
    "    # Decode the prediction to get human-readable labels. \n",
    "    # 'top=1' means we only want the top 1 prediction, i.e., the label with the highest probability.\n",
    "    decoded_pred = tf.keras.applications.efficientnet.decode_predictions(prediction, top = 1)[0][0][1]\n",
    "    \n",
    "    # Return a descriptive string with the predicted label.\n",
    "    return f'I think the image contains {decoded_pred}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function by describing an image captured by our camera.\n",
    "# describe_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_gen():\n",
    "    \"\"\"\n",
    "    A generator function that keeps producing the next integer.\n",
    "    Starts from 1 and increments by 1 for each subsequent call.\n",
    "    \n",
    "    Yields:\n",
    "        int: The next integer in the sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a variable 'num' with the value 1. This will be our starting point.\n",
    "    num = 1\n",
    "    \n",
    "    # A never-ending loop. Since there's no condition to break the loop, it will keep running indefinitely.\n",
    "    while True:\n",
    "        \n",
    "        # 'yield' essentially returns 'num' to the caller and then, the next time the generator \n",
    "        # is called, it resumes from right after this point.\n",
    "        yield num\n",
    "        \n",
    "        # Increment the value of 'num' by 1.\n",
    "        num += 1\n",
    "\n",
    "# Create an instance of the generator. This doesn't start the generator, but prepares it to be used.\n",
    "gen = no_gen()\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"\n",
    "    Converts the provided text into speech using Google Text-to-Speech (gTTS) \n",
    "    and plays the resulting audio using the pygame library.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be converted into speech.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the gTTS library to convert the provided text into speech. \n",
    "    # The 'lang' parameter specifies that we want to use English for the speech synthesis.\n",
    "    tts = gTTS(text= text, lang= 'en')\n",
    "    \n",
    "    # Get the next number from our generator. This ensures each audio file has a unique name.\n",
    "    resp_no = next(gen)\n",
    "    \n",
    "    # Format the audio file name using the obtained number.\n",
    "    audio_file = f'response{resp_no}.mp3'\n",
    "    \n",
    "    # Save the speech audio into an mp3 file.\n",
    "    tts.save(audio_file)\n",
    "    \n",
    "    # Initialize the pygame mixer. This prepares pygame to play audio.\n",
    "    pygame.mixer.init()\n",
    "    \n",
    "    # Load the saved audio file into pygame.\n",
    "    pygame.mixer.music.load(audio_file)\n",
    "    \n",
    "    # Play the loaded audio.\n",
    "    pygame.mixer.music.play()\n",
    "    \n",
    "    # This loop ensures the program doesn't proceed until the audio is finished playing.\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function by making it speak \"how are you\".\n",
    "# speak(\"how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "1. `no_gen()`: This is a generator function. It's a special type of function that can pause its execution and resume from where it left off. Here, it's used to produce an endless sequence of integers. It's a neat way of generating unique identifiers without having to keep track of state outside the function.\n",
    "\n",
    "2. `gen = no_gen()`: Creates a generator object. Now, every time you call `next(gen)`, you'll get the next number in the sequence.\n",
    "\n",
    "3. `speak(text)`: This function is responsible for converting the given text into speech and then playing that speech. It uses the Google Text-to-Speech (gTTS) service to create an audio file from the text, and then the `pygame` library to play that audio. The use of the generator function ensures that each generated audio file has a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function to drive the entire operation.\n",
    "    Listens to the user, performs actions accordingly, and responds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This function call captures the user's voice and tries to convert it into text.\n",
    "    user_input = listen_to_user()\n",
    "    \n",
    "    # Checking if the system was able to successfully capture and convert voice to text.\n",
    "    if user_input:\n",
    "        \n",
    "        # Check if the user wants to describe an image.\n",
    "        if 'describe the image' in user_input.lower():\n",
    "            \n",
    "            # Use the earlier described function to capture and describe an image.\n",
    "            response = describe_image()\n",
    "        \n",
    "        # Check if the user wants to play a song.\n",
    "        elif 'play my song' in user_input.lower():\n",
    "            \n",
    "            # Use the webbrowser library to open YouTube in the default browser.\n",
    "            webbrowser.open('https://www.youtube.com/')\n",
    "            response = \"playing your song\"\n",
    "        \n",
    "        # Print out what the assistant will respond with.\n",
    "        print('Assistant Resp : ', response)\n",
    "        \n",
    "        # Use the `speak` function to convert the response into speech and play it.\n",
    "        speak(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment the following line to run the main function and start the voice assistant.\n",
    "# main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
